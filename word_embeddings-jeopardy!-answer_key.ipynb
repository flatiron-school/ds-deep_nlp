{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NLP - Word Embeddings\n",
    "\n",
    "Think back to NLP as we've understood it so far.\n",
    "\n",
    "If we've had some luck with NLP modeling, likely with a NaiveBayes algorithm, we were able to illustrate some correlations between words and some other feature of interest.\n",
    "\n",
    "But to whatever extent our models were able to make connections and pick up on correlations, they did this *without any understanding of the **meaning** of the words in question*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make sense of meaning for computational purposes by thinking about meaning in terms of similarity, i.e. thinking about meaning *holistically*.\n",
    "\n",
    "Q. Is there any precedent for this way of thinking about meaning? <br/>\n",
    "A. [Yes](https://plato.stanford.edu/entries/meaning-holism/#ArgForMeaHol)\n",
    "\n",
    "So what will this look like for us?\n",
    "\n",
    "*Remember cosine similarity?*\n",
    "\n",
    "$\\rightarrow$We'll have much the same idea here: Associate each word with values along particular dimensions in a multi-dimensional space. If we had a dimension for *softness*, for example, then pillows and marshmallows would score higher on it than rocks and bricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Gensim? See [here](https://en.wikipedia.org/wiki/Gensim) and [here](https://radimrehurek.com/gensim/). But, basically, gensim is a package with lots of topic-modeling and NLP tools, inlcuding Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "\n",
    "with open('JEOPARDY_QUESTIONS1.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216930"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'HISTORY',\n",
       " 'air_date': '2004-12-31',\n",
       " 'question': \"'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'\",\n",
       " 'value': '$200',\n",
       " 'answer': 'Copernicus',\n",
       " 'round': 'Jeopardy!',\n",
       " 'show_number': '4680'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first element in our list\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words do we have in this first question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'For\",\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life,',\n",
       " 'Galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " \"man's\",\n",
       " \"theory'\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['question'].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['question'].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words do we have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169994"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 0\n",
    "for clue in data:\n",
    "    length += len(clue['question'].split(' '))\n",
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Word2Vec requires that our text have the form of a list of 'sentences', where each sentence is itself a list of words. How can we put our _Jeopardy!_ clues in that shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for clue in data:\n",
    "    sentence = clue['question'].translate(str.maketrans('', '',\n",
    "                                                        string.punctuation)).split(' ')\n",
    "    \n",
    "    new_sent = []\n",
    "    for word in sentence:\n",
    "        new_sent.append(word.lower())\n",
    "    \n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " 'galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " 'mans',\n",
       " 'theory']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = []\n",
    "for clue in text:\n",
    "    lemmatized.append([lemma.lemmatize(word) for word in clue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'year',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " 'galileo',\n",
       " 'wa',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " 'man',\n",
       " 'theory']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Bag of Words vs. Skipgram\n",
    "\n",
    "<a href=\"https://www.researchgate.net/figure/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models_fig1_281812760\"><img src=\"https://www.researchgate.net/profile/Wang_Ling/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png\" alt=\"Illustration of the Skip-gram and Continuous Bag-of-Word (CBOW) models.\"/></a>\n",
    "\n",
    "[More on Skipgram](https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec supports both **continuous bag of words** and **skipgram** models. [This post](https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314) is helpful on their structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model is simply a matter of\n",
    "# instantiating a Word2Vec object.\n",
    "\n",
    "model = gensim.models.Word2Vec(lemmatized, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See doc for note about seeding and reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299626, 15849970)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train, call 'train()'!\n",
    "\n",
    "model.train(text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169994"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking word count\n",
    "\n",
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `model.wv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x15705a940>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The '.wv' attribute stores the word vectors\n",
    "\n",
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35933653, -0.16895719,  0.08698016, -0.2724295 ,  0.03410744,\n",
       "       -0.876136  ,  0.22552657,  0.18308343,  0.08276779,  0.12514946,\n",
       "       -0.3964169 ,  0.03763022, -0.56658256,  0.2665006 , -0.21136418,\n",
       "        0.09357793,  0.81534517,  0.01875736, -0.52860904,  0.1600756 ,\n",
       "       -0.09026229,  0.18312268, -0.14695741, -0.09544276,  0.46544105,\n",
       "        0.67673206,  0.38662106, -0.30341986, -0.28568813, -0.27198285,\n",
       "       -0.10253334,  0.8163238 , -0.04682573,  0.4146795 ,  0.25085577,\n",
       "       -0.29462567, -0.16199362, -0.38281444,  0.09366592, -0.07808338,\n",
       "        0.50254357, -0.34361532, -0.21382678, -0.07930967,  0.16761082,\n",
       "        0.5728547 ,  0.11123757,  0.48029777, -0.10854993, -0.3600292 ,\n",
       "        0.28469074,  0.00913351, -0.22246398, -0.55484813,  0.39430794,\n",
       "        0.09160047, -0.22096094,  0.7419396 , -0.01934808, -0.17086174,\n",
       "       -0.25481266,  0.5247727 , -0.3014785 , -0.02293869, -0.20031291,\n",
       "       -0.23553422, -0.29731867,  0.07297182, -0.17568329,  0.06389911,\n",
       "        0.2963827 ,  0.39931175, -0.18140872,  0.39977783, -0.49252534,\n",
       "       -0.33794412, -0.22760597, -0.21653914, -0.17879806, -0.15671766,\n",
       "       -0.01234196,  0.1373972 ,  0.23371027,  0.4208116 , -0.34202123,\n",
       "       -0.42146215,  0.27000868,  0.18583125, -0.07925742,  0.20297094,\n",
       "       -0.3178881 , -0.17671669, -0.35222536, -0.0567392 , -0.11104529,\n",
       "       -0.03293213, -0.13565361, -0.13454437, -0.13980947, -0.29738933],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vectors are keyed by the words\n",
    "\n",
    "model.wv['child']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.wv` methods\n",
    "#### `most_similar()` and `similarity()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ceramic', 0.7321969866752625),\n",
       " ('cabriole', 0.714465320110321),\n",
       " ('pottery', 0.6944854259490967),\n",
       " ('linen', 0.6927464008331299),\n",
       " ('fastener', 0.6918306946754456),\n",
       " ('flooring', 0.6882908344268799),\n",
       " ('artwork', 0.6843509674072266),\n",
       " ('chippendale', 0.6841990947723389),\n",
       " ('decorative', 0.6809938549995422),\n",
       " ('mannequin', 0.6777957677841187)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('furniture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73219705"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('furniture', 'ceramic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carnivore', 0.8239153623580933),\n",
       " ('cheetah', 0.8141511678695679),\n",
       " ('predatory', 0.7844293117523193),\n",
       " ('pachyderm', 0.7825244069099426),\n",
       " ('arachnid', 0.777582049369812),\n",
       " ('reptile', 0.7768771648406982),\n",
       " ('shorthaired', 0.7749311923980713),\n",
       " ('rodent', 0.7739750742912292),\n",
       " ('giraffe', 0.7706668376922607),\n",
       " ('scavenger', 0.7691413164138794)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat', 'animal', 'pet', 'mammal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following 'equations':\n",
    "\n",
    "King + Woman - Man = x\n",
    "\n",
    "Brother + Woman - Man = y\n",
    "\n",
    "What values would you suggest for x and y here?\n",
    "\n",
    "Clearly, getting good answers to these equations depends on understanding the *meanings* of the underlying words.\n",
    "\n",
    "Or does it? The `most_similar()` method takes a 'negative' parameter as well as a 'positive' one, so we can consult our trained word vectors to see how they would answer these questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rodent', 0.3702376186847687),\n",
       " ('sheep', 0.36358124017715454),\n",
       " ('breed', 0.35542649030685425),\n",
       " ('vulture', 0.35324373841285706),\n",
       " ('insect', 0.3531191647052765),\n",
       " ('dog', 0.34904420375823975),\n",
       " ('extinction', 0.34766697883605957),\n",
       " ('lizard', 0.3446741998195648),\n",
       " ('creature', 0.3275783658027649),\n",
       " ('domesticated', 0.3261025547981262)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['cat', 'animal'], negative='pet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emperor', 0.26721689105033875),\n",
       " ('throne', 0.26415058970451355),\n",
       " ('empress', 0.22536341845989227),\n",
       " ('spaniard', 0.22355356812477112),\n",
       " ('slave', 0.2168462574481964),\n",
       " ('ruler', 0.21443983912467957),\n",
       " ('queen', 0.20846901834011078),\n",
       " ('medici', 0.20117774605751038),\n",
       " ('conqueror', 0.1999264359474182),\n",
       " ('aristotle', 0.19925367832183838)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king', 'woman'], negative='man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.2738255560398102),\n",
       " ('husband', 0.2578567564487457),\n",
       " ('motherinlaw', 0.2373926043510437),\n",
       " ('slave', 0.23261277377605438),\n",
       " ('nymph', 0.2305372804403305),\n",
       " ('son', 0.2225310206413269),\n",
       " ('jacob', 0.2190614640712738),\n",
       " ('odysseus', 0.2141270637512207),\n",
       " ('daughter', 0.21392333507537842),\n",
       " ('jealous', 0.21081501245498657)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['brother', 'woman'], negative='man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pageant', 0.61308354139328),\n",
       " ('monorail', 0.610979437828064),\n",
       " ('fargo', 0.6104167103767395),\n",
       " ('whitewater', 0.6084681749343872),\n",
       " ('coanchor', 0.5854692459106445),\n",
       " ('firecracker', 0.5806131362915039),\n",
       " ('brisbane', 0.5774937868118286),\n",
       " ('shannon', 0.5772475600242615),\n",
       " ('tyra', 0.574211061000824),\n",
       " ('dogpatch', 0.5650832653045654)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marianas', 0.6302838325500488),\n",
       " ('britain', 0.6298997402191162),\n",
       " ('myanmar', 0.623246431350708),\n",
       " ('manitoba', 0.6185489296913147),\n",
       " ('klm', 0.6138970851898193),\n",
       " ('saskatchewan', 0.6122548580169678),\n",
       " ('zambia', 0.6085365414619446),\n",
       " ('albania', 0.6065628528594971),\n",
       " ('mozambique', 0.606482744216919),\n",
       " ('canadian', 0.6046032309532166)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sophocles', 0.7020344734191895),\n",
       " ('shakespearean', 0.68357914686203),\n",
       " ('euripides', 0.6722855567932129),\n",
       " ('macbeth', 0.6563365459442139),\n",
       " ('falstaff', 0.6521875858306885),\n",
       " ('hamlet', 0.6492114067077637),\n",
       " ('moliere', 0.6449787616729736),\n",
       " ('ibsen', 0.6441601514816284),\n",
       " ('hellman', 0.6436213254928589),\n",
       " ('laertes', 0.6376091241836548)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kinnear', 0.8460290431976318),\n",
       " ('conner', 0.8010389804840088),\n",
       " ('shoeless', 0.7997528314590454),\n",
       " ('bebe', 0.7965857982635498),\n",
       " ('kerri', 0.7933998107910156),\n",
       " ('connors', 0.7874476909637451),\n",
       " ('shaun', 0.7821326851844788),\n",
       " ('waterstona', 0.7787505388259888),\n",
       " ('abduljabbar', 0.773412823677063),\n",
       " ('hamlisch', 0.7722470760345459)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('greg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('madison', 0.6711403131484985),\n",
       " ('prescott', 0.6692360043525696),\n",
       " ('lincoln', 0.6623405814170837),\n",
       " ('mifflin', 0.6616520285606384),\n",
       " ('romney', 0.6580953598022461),\n",
       " ('lubbock', 0.657781183719635),\n",
       " ('colby', 0.6562408208847046),\n",
       " ('seward', 0.6557228565216064),\n",
       " ('polk', 0.65288245677948),\n",
       " ('judiciary', 0.6506866216659546)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('jefferson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dc', 0.8224882483482361),\n",
       " ('dca', 0.6604635715484619),\n",
       " ('hw', 0.6486342549324036),\n",
       " ('newseum', 0.6236661672592163),\n",
       " ('p3', 0.6230477690696716),\n",
       " ('statuary', 0.6045047640800476),\n",
       " ('rotunda', 0.5910694003105164),\n",
       " ('gw', 0.5870948433876038),\n",
       " ('arlington', 0.5859220027923584),\n",
       " ('lincoln', 0.5850906372070312)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('washington')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dictator', 0.24386319518089294),\n",
       " ('ruled', 0.23667289316654205),\n",
       " ('assassinated', 0.20818036794662476),\n",
       " ('1894', 0.20603422820568085),\n",
       " ('rebellion', 0.20375829935073853),\n",
       " ('slave', 0.2037426233291626),\n",
       " ('premier', 0.19761990010738373),\n",
       " ('emperor', 0.19710686802864075),\n",
       " ('haile', 0.19596856832504272),\n",
       " ('occupied', 0.1949566900730133)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['president', 'germany'], negative='usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shah', 0.24299493432044983),\n",
       " ('dictator', 0.23337674140930176),\n",
       " ('medici', 0.22955113649368286),\n",
       " ('conquest', 0.22408586740493774),\n",
       " ('ruled', 0.22132796049118042),\n",
       " ('assassinated', 0.21796737611293793),\n",
       " ('exile', 0.21268150210380554),\n",
       " ('occupied', 0.2110612690448761),\n",
       " ('frenchman', 0.21021825075149536),\n",
       " ('spaniard', 0.20750625431537628)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['president', 'france'], negative='usa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'doesnt_match()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gdamico/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py:876: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['breakfast', 'lunch', 'frog', 'food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bush'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['tree', 'flower', 'bush', 'plant', 'toothbrush'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toothbrush'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['tree', 'flower', 'plant', 'toothbrush'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'closer_than()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iv', 'ix', 'olaf']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which words are closer to 'king' than 'queen' is?\n",
    "\n",
    "model.wv.closer_than('king', 'queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'distance()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this it will make more sense to\n",
    "# normalize our vectors.\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1920928955078125e-07"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('king', 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4816325902938843"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('joy', 'happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'evaluate_word_analogies()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate_word_analogies()` method takes in a string of quadruples, properly formatted (see [here](https://radimrehurek.com/gensim/models/keyedvectors.html)), and returns a list of dictionaries. Each dictionary has two keys: 'correct' and 'incorrect', the values for which are lists of the analogies that the model correctly or incorrectly predicted.\n",
    "\n",
    "Check out [this text file](https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gdamico/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "relatives = model.wv.evaluate_word_analogies(\n",
    "    \"\"\"https://raw.githubusercontent.com/\"\"\"\\\n",
    "    \"\"\"nicholas-leonard/word2vec/master/questions-words.txt\"\"\")[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relatives['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relatives['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'DAD', 'MOM'),\n",
       " ('BOY', 'GIRL', 'HE', 'SHE'),\n",
       " ('BOY', 'GIRL', 'HIS', 'HER'),\n",
       " ('BOY', 'GIRL', 'KING', 'QUEEN'),\n",
       " ('BOY', 'GIRL', 'MAN', 'WOMAN')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatives['correct'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'BROTHER', 'SISTER'),\n",
       " ('BOY', 'GIRL', 'FATHER', 'MOTHER'),\n",
       " ('BOY', 'GIRL', 'GRANDFATHER', 'GRANDMOTHER'),\n",
       " ('BOY', 'GIRL', 'GRANDPA', 'GRANDMA'),\n",
       " ('BOY', 'GIRL', 'GRANDSON', 'GRANDDAUGHTER')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatives['incorrect'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Parser\n",
    "https://nlp.stanford.edu/software/lex-parser.shtml <br/>\n",
    "http://nlp.stanford.edu:8080/parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
